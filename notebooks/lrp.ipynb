{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fa1a1f3-806e-4588-9bc9-30d9bfe50fc8",
   "metadata": {},
   "source": [
    "# Explainable AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7999fc3d-a418-44f9-aeba-fd34f8ac25c7",
   "metadata": {},
   "source": [
    "*Still under construction*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a3c301-2d2a-4513-b3d2-b9351de5dada",
   "metadata": {},
   "source": [
    "A big problem of artificial neural networks is that they are commonly considered as \"black boxes\". This means that we generally only know about the input and the output but what happens in between is unknown. While the classification accuracies may be high we do not know how they got to their decisions. It is possible that a neural network perfected to distinguish certain patterns in the data which have nothing to do with the actual thing. \n",
    "\n",
    "This is where explainable AI comes into play. The idea behind is to create a map of scores or measures that indicate the importance of a given feature/pixel/voxel in the input space. \n",
    "\n",
    "Here is an illustration of the [layer-wise relevance propagation](https://www.hhi.fraunhofer.de/en/departments/ai/technologies-and-solutions/layer-wise-relevance-propagation.html) (LRP) algorithm developed in the lab of Wojciech Samek and Sebastian Lapuschkin:\n",
    "\n",
    "![LRP illustration](https://www.hhi.fraunhofer.de/fileadmin/_processed_/b/9/csm_lrp-algorithm_044c31eb4a.png)\n",
    "\n",
    "The LRP algorithm is already implemented in many python toolboxes. In case you want to check them out for yourself here is a collection  of Explainable AI toolboxes: \n",
    "* [iNNvestigate](https://github.com/albermax/innvestigate)\n",
    "* [Zennit](https://github.com/chr5tphr/zennit)\n",
    "* [Captum](https://github.com/pytorch/captum)\n",
    "* SHAP, see [A](https://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf) and [B](https://github.com/slundberg/shap)\n",
    "\n",
    "Here we will take a look at the [Zennit](https://github.com/chr5tphr/zennit) toolbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9f1b1270-816a-45a5-b799-8030b2f03af0",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../../fmriDEEP'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dcecec22-7019-4c62-b6e6-a770117f355b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from _core.networks.ConvNets import Simple2dCnnClassifier\n",
    "from _core.utils.tools import compute_accuracy\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from zennit import composites\n",
    "from torchvision.datasets import MNIST, FashionMNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# this variable contains information whether a GPU can be used for training. If not, we automatically use the CPU.\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb84d01c-8835-4f66-b458-00da4037333f",
   "metadata": {},
   "source": [
    "<p align=\"justify\">To ensure reproducibility we set the random seed for all sorts of randomizer tools.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "330a32c4-aed5-4b5f-ad16-4d49147d70ce",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "# set the random seed for reproducibility\n",
    "def set_random_seed(seed):\n",
    "    import random \n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    g = torch.Generator() # can be used in pytorch dataloaders for reproducible sample selection when shuffle=True\n",
    "    g.manual_seed(seed)\n",
    "    \n",
    "    return g\n",
    "\n",
    "g = set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1865bd85-600b-46fb-9f8e-31d711544f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to run the train function independent of the \n",
    "# supplied network class\n",
    "# when done training, save the model at \"save_name\"\n",
    "def train_net(model, n_epochs, lr=.01, device=torch.device(\"cpu\"), save_name=None):\n",
    "    # set some variables here, such that we can create pretty plots\n",
    "    # we could make these return parameters\n",
    "    train_loss = np.zeros(n_epochs)\n",
    "    test_loss = np.zeros_like(train_loss)\n",
    "    train_acc = np.zeros_like(train_loss)\n",
    "    test_acc = np.zeros_like(train_loss)\n",
    "\n",
    "    # loop for the above set number of epochs\n",
    "    for epoch in range(0, n_epochs):\n",
    "\n",
    "        # THIS IS WHERE THE MAGIC HAPPENS\n",
    "        # calling the model.fit() function will execute the 'standard_train' function as defined above.\n",
    "        train_loss[epoch], train_stats = model.fit(dl_train, lr=lr, device=device)\n",
    "        train_acc[epoch] = compute_accuracy(train_stats[:, -1], train_stats[:, -2])\n",
    "\n",
    "        # for validating or testing set the network into evaluation mode such that layers like dropout are not active\n",
    "        with torch.no_grad():\n",
    "            test_loss[epoch], test_stats = model.fit(dl_test, device=device, train=False)\n",
    "            test_acc[epoch] = compute_accuracy(test_stats[:, -1], test_stats[:, -2])\n",
    "\n",
    "        print('epoch=%03d, train_loss=%1.3f, train_acc=%1.3f, test_loss=%1.3f, test_acc=%1.3f' % \n",
    "             (epoch, train_loss[epoch], train_acc[epoch], test_loss[epoch], test_acc[epoch]))\n",
    "\n",
    "    model.save(save_name, save_full=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8091553f-f3f4-4ecf-bf00-5af787ec58c2",
   "metadata": {},
   "source": [
    "For this chapter we will use the ```Fashion MNIST``` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b8c0c1c7-b252-4fbb-84c0-2bfd955ff6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = FashionMNIST(root=\"data\",train=True,download=True,transform=ToTensor())\n",
    "dl_train = DataLoader(training_data, batch_size=256, shuffle=True)\n",
    "\n",
    "test_data = FashionMNIST(root=\"data\",train=False,download=True,transform=ToTensor())\n",
    "dl_test = DataLoader(test_data, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd35a62-8533-4e59-89c6-7f0c29d31282",
   "metadata": {},
   "source": [
    "Let's look at an example image of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b05013f1-e9f3-4154-b9e5-db3347165e9e",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show an example of the Fashion MNIST dataset\n",
    "plt.imshow(test_data.data[0, :, :], cmap='Greys');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e971541-c873-4026-83a2-2ba0e94fe7b2",
   "metadata": {},
   "source": [
    "## Feature relevance in 2d Fashion MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bf6cbf-9a64-4309-a013-f4b7253108f1",
   "metadata": {},
   "source": [
    "As we have learned in previous chapters it is quite easy to create a new ```Simple2dCnnClassifier```. After we created the network we train it for some epochs. \n",
    "\n",
    "I have already done the training, so I am simply loading the save state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "70b4b7d3-7a92-45f3-b2ea-db66d49d62fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Simple2dCnnClassifier((28, 28), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e3e17d6f-f984-4be0-9632-411b54246c68",
   "metadata": {
    "tags": [
     "hide_output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping the training procedure\n"
     ]
    }
   ],
   "source": [
    "%%script echo Skipping the training procedure\n",
    "train_net(model, 10, save_name='my_cnn_fashionmnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "00a49a84-0aae-4c31-b7fe-166829e8072e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the already trained model\n",
    "model = torch.load('my_cnn_fashionmnist/model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50565680-ffd4-4926-a633-b2a04b766917",
   "metadata": {},
   "source": [
    "Unfortunately, I did not yet get around to make functions for the \"explaining\" code from [Zennit](https://github.com/chr5tphr/zennit). \n",
    "\n",
    "It will get easier but for now let us go through the code step by step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d0c9ef1d-128d-4d16-9472-c79f4ce2925a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "shape = (1, 1, 28, 28)\n",
    "\n",
    "# Zennit provides multiple options rules for computation.\n",
    "# In a paper, Montavon, et. al (https://iphome.hhi.de/samek/pdf/MonXAI19.pdf) \n",
    "# showed that a composite of different rules e.g., the epsilon gamma rules works best \n",
    "# for neural networks that are a combination of convolutional and linear layers.\n",
    "composite_kwargs = {\n",
    "    'low': 0 * torch.ones(*shape, device=torch.device(\"cpu\")),  # the lowest and ...\n",
    "    'high': 1 * torch.ones(*shape, device=torch.device(\"cpu\")),  # the highest pixel value for ZBox\n",
    "}\n",
    "# In this line I am telling zennit what kind of rule I want to use and supply\n",
    "# some arguments. \n",
    "attributor = composites.COMPOSITES['epsilon_gamma_box'](**composite_kwargs)\n",
    "\n",
    "# We can turn off the gradients for each layer, we wont need them.\n",
    "# This makes the foward pass faster etc.\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# simply the image from a batch to pick for plotting\n",
    "pick_img = 1\n",
    "    \n",
    "fig, axes = plt.subplots(1, 10, figsize=(20, 8), sharex=True, sharey=True)\n",
    "\n",
    "# Now we activate the attributer context in which we can use Zennit functionality.\n",
    "# this allows us accumulate the relevance scores in the input space by calling\n",
    "# the torch.autograd.backward function.\n",
    "# Zennit sort of \"overrides\" the rules what to compute in the backward pass.\n",
    "# This results in relevance maps in the input space.\n",
    "with attributor.context(model) as modified_model:\n",
    "    ctr = 0\n",
    "    for i, (data, target) in enumerate(dl_test):\n",
    "        # we do not want the actuall input to change\n",
    "        data_with_grad = data.clone()\n",
    "        \n",
    "        # the copied input needs gradients so we can change them\n",
    "        # during the backward pass\n",
    "        data_with_grad.requires_grad_()\n",
    "\n",
    "        # the label in one-hot encoding\n",
    "        output_relevance = torch.eye(10, device=torch.device(\"cpu\"))[target]\n",
    "\n",
    "        # forward pass with the zennit-modified model\n",
    "        out = modified_model(data_with_grad)\n",
    "        \n",
    "        # what did the model predict\n",
    "        predicted = np.argmax(out.detach().numpy(), axis=1)\n",
    "        \n",
    "        # magic :)\n",
    "        torch.autograd.backward(out, output_relevance)\n",
    "        \n",
    "        # plot some images (relevance overlayed on the original)\n",
    "        axes[ctr].imshow(data[pick_img, :, :].squeeze().squeeze().cpu().numpy(), cmap='Greys')\n",
    "        axes[ctr].imshow(data_with_grad.grad[pick_img,:,:].squeeze().squeeze().cpu().numpy(), cmap='coolwarm', alpha=.5)\n",
    "        axes[ctr].set_title(f'{predicted[pick_img]}-{target[pick_img]}')\n",
    "\n",
    "        ctr += 1\n",
    "        if ctr > 9:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2403d455-b897-47f6-b34c-7868160ebe1c",
   "metadata": {},
   "source": [
    "## Explain brain data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caba42ee-1403-4fbb-8c13-85cc083cf333",
   "metadata": {},
   "source": [
    "So far we have not talked about brain data. That is quite unfortunate but it is always good to start with the easier tasks :).\n",
    "\n",
    "Let's say we want to train a neural network that can identify brain states, or important brain areas involved in a task, based on 3-dimensional functional magnetic resonance imaging (fMRI) data. For example, we want to know...\n",
    "* ... if we can classify between brain patterns of hand, foot, and tongue movement\n",
    "* ... see which brain areas the neural network deems most relevant for its decision\n",
    "\n",
    "With the knowledge and the tools we now have at our disposal we can do the following:\n",
    "\n",
    "1. Write or use a Dataset that can handle Nifti data (e.g., use the ```_core.datasets.NiftiDataset```)\n",
    "2. Create a neural network that can handle 3-dimensional fMRI data (e.g., the ```BrainStateClassifier3d```\n",
    "3. Train the network using the ```BrainStateClassifier3d.fit()```method\n",
    "4. Check the classification performance of the network\n",
    "5. Use the layer-wise relevance propagation (LRP) algorithm to identify important voxels\n",
    "6. Look at the LRP maps :)\n",
    "\n",
    "I have already done steps 1-4 so these steps are pretty much skipped. But it is no different than training on the MNIST datasets, so you should be quite familiar with the process by now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5918a6de-85d9-4bb2-9f8d-82562ffe7ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltools.data import Brain_Data\n",
    "from _core.utils.train_fns import standard_train\n",
    "from _core.networks.ConvNets import BrainStateClassifier3d\n",
    "from _core.utils.datasets import NiftiDataset\n",
    "import _core.utils.tools as utils\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "75357c0e-40c2-40bb-a6ee-fbf4dbb910e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. create the datasets and dataloaders\n",
    "labels = ['handleft', 'handright', 'footleft', 'footright', 'tongue']\n",
    "dl_train = DataLoader(NiftiDataset(\n",
    "    'data/brain_data/train', labels, 150, DEVICE, transform=utils.ToTensor()), \n",
    "    batch_size=4, shuffle=True, generator=g\n",
    ")\n",
    "dl_test = DataLoader(NiftiDataset(\n",
    "    'data/brain_data/test', labels, 20, DEVICE, transform=utils.ToTensor()), \n",
    "    batch_size=4, shuffle=True, generator=g\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8f6d31df-075a-4881-9e7e-aa571341a799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already ran. Skipping to save time.\n"
     ]
    }
   ],
   "source": [
    "%%script echo Already ran. Skipping to save time.\n",
    "# 2. create the 3d BrainStateClassifier\n",
    "model = BrainStateClassifier3d((91, 109, 91), len(labels)).to(DEVICE)\n",
    "\n",
    "# 3. train the network\n",
    "train_net(model, 100, lr=.00001, device=DEVICE, save_name='motor-mapper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ad68836a-d27e-40ae-8be0-25464e8d6ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already ran. Skipping to save time.\n"
     ]
    }
   ],
   "source": [
    "%%script echo Already ran. Skipping to save time.\n",
    "# 5. Execute the LRP algorithm and save the resulting \n",
    "# LRP maps somewhere.\n",
    "shape = (1, 1, 91, 109, 91)\n",
    "\n",
    "composite_kwargs = {\n",
    "    'low': -1 * torch.ones(*shape, device=torch.device(\"cpu\")),  # the lowest and ...\n",
    "    'high': 1 * torch.ones(*shape, device=torch.device(\"cpu\")),  # the highest pixel value for ZBox\n",
    "}\n",
    "\n",
    "attributor = composites.COMPOSITES['epsilon_gamma_box'](**composite_kwargs)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# create the composite context outside the main loop, such that the canonizers and hooks do not\n",
    "# need to be registered and removed for each step.\n",
    "with attributor.context(model) as modified_model:\n",
    "    for j in range(len(labels)):\n",
    "        dl = DataLoader(\n",
    "            NiftiDataset('data/brain_data/test', [labels[j]], 20, torch.device(\"cpu\"), 3),\n",
    "            batch_size=20, shuffle=False, num_workers=0\n",
    "        )\n",
    "\n",
    "        for i, (volume, target) in enumerate(dl):\n",
    "            data_norm = (volume.float().to(DEVICE))\n",
    "            data_norm.requires_grad_()\n",
    "\n",
    "            # one-hot encoding of the target labels\n",
    "            output_relevance = torch.eye(model.config['n_classes'], device=torch.device(\"cpu\"))[target]\n",
    "            out = modified_model(data_norm)\n",
    "            # a simple backward pass will accumulate the relevance in data_norm.grad\n",
    "            torch.autograd.backward((out,), (output_relevance,))\n",
    "            indi_lrp = np.moveaxis(data_norm.grad.squeeze().detach().numpy(), 0, -1)\n",
    "            avg_lrp = indi_lrp.mean(axis=-1)\n",
    "\n",
    "        utils.save_in_mni(indi_lrp, os.path.join('motor-mapper', 'lrp_%s.nii.gz' % labels[j]))\n",
    "        utils.save_in_mni(avg_lrp, os.path.join('motor-mapper', 'lrp_avg_%s.nii.gz' % labels[j]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b8ae41-9329-4fa8-9820-05bc5cc21e8b",
   "metadata": {},
   "source": [
    "You can now look at the average LRP maps for the classes (volume slider) or..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6d025951-d029-4f6e-8d35-1e1942a3fdd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/phisei/opt/anaconda3/envs/fmrideep/lib/python3.8/site-packages/nltools/plotting.py:77: UserWarning: Percentile thresholding ignores brain mask. Results are likely more liberal than you expect (e.g. with non-interactive plotting)!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac80247a55914d78996ed10b79913967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatText(value=99.0, description='Threshold'), IntSlider(value=0, continuous_update=Fal…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "files = sorted(glob.glob('motor-mapper/lrp_avg*.nii.gz'))\n",
    "dat = Brain_Data(files)\n",
    "dat.iplot(threshold=\"99%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3d4821-7a8c-470e-84f4-8e22419ad79a",
   "metadata": {},
   "source": [
    "...you check out the subject specific LRP maps for a given class (e.g., the \"handleft\" class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "53fe4e19-79a8-4833-aecf-a42276a7eee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/phisei/opt/anaconda3/envs/fmrideep/lib/python3.8/site-packages/nltools/plotting.py:77: UserWarning: Percentile thresholding ignores brain mask. Results are likely more liberal than you expect (e.g. with non-interactive plotting)!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ece6a8f8f93441679cdd589ae1043c74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatText(value=95.0, description='Threshold'), IntSlider(value=0, continuous_update=Fal…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file = 'motor-mapper/lrp_handleft.nii.gz'\n",
    "dat = Brain_Data(file)\n",
    "dat.iplot(threshold=\"95%\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725be9a3-411b-4f18-801f-201d4aad4935",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
