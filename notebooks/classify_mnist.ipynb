{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3b51b26-0911-4d4d-a0a3-e4fb3cca57ff",
   "metadata": {},
   "source": [
    "# Classifying MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931f7a56-68f9-4737-9a23-7f5b9089670a",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../../fmriDEEP'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b51cc1-75c6-41d1-a188-e17938618eff",
   "metadata": {},
   "source": [
    "<p align=\"justify\">We can now import the necessary packages and tools to train a simple linear classifier with PyTorch and my custom code.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddec548-d204-4c62-869a-28a526af288c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other packages\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchinfo import summary\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "from torchvision.datasets import MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# custom packages\n",
    "from _core.utils.train_fns import standard_train # this is a predefined training function we will use\n",
    "from _core.utils.tools import compute_accuracy   # a custom function to compute the accuracy scores\n",
    "from _core.networks.LinearNets import SimpleLinearModel\n",
    "from _core.networks.ConvNets import Simple2dCnnClassifier\n",
    "\n",
    "# this variable contains information whether a GPU can be used for training. If not, we automatically use the CPU.\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b795577-b5ab-464c-b88f-67f6bfb0f479",
   "metadata": {},
   "source": [
    "<p align=\"justify\">To ensure reproducibility we set the random seed for all sorts of randomizer tools.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb96f53-c90d-4d3b-b85e-a775988e7998",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "# set the random seed for reproducibility\n",
    "def set_random_seed(seed):\n",
    "    import random \n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    g = torch.Generator() # can be used in pytorch dataloaders for reproducible sample selection when shuffle=True\n",
    "    g.manual_seed(seed)\n",
    "    \n",
    "    return g\n",
    "\n",
    "g = set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703bf351-45c2-4a98-84b8-07bfc5957da9",
   "metadata": {},
   "source": [
    "## Download and inspect the MNIST data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf7d33f-07a3-41cb-bbc0-88921a925640",
   "metadata": {},
   "source": [
    "The easiest benchmark test for any neural network is to test its classification performance on the MNIST dataset.\n",
    "This dataset contains thousands of hand-written digit exemplars. \n",
    "As it is quite easy to use, we will employ it here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c6dc92-cddf-42a9-9599-724b677c4ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = MNIST('./data/', train=True, download=True if not os.path.exists('./data/MNIST') else False, transform=ToTensor())\n",
    "mnist_test = MNIST('./data/', train=False, download=False, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec300cc-bc7b-4718-be3d-4b43a4f74f57",
   "metadata": {},
   "source": [
    "Let us look at some examples from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a53a6d-b85d-4aa6-9ba2-c8269a7822bc",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,4, figsize=(12,6), sharex=True, sharey=True)\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(mnist_train.data[i], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3add7524-26bf-475c-a9d0-f304eae2b4df",
   "metadata": {},
   "source": [
    "<p align=\"justify\">What you also always should do is look at your data or something that data scientists would call 'Exploratory Data Analysis' (EDA). EDA usually can be quite extensive and they are- for now -too much for the scope of this book. \n",
    "\n",
    "We can now turn our attention to creating the network and setting up the training procedure.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ca5dbf-6ee4-47c1-8c88-04d3386bf7df",
   "metadata": {},
   "source": [
    "## Creating a simple linear neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cfface-4cf0-431b-855e-53581b6764fb",
   "metadata": {},
   "source": [
    "<p align=\"justify\">As demonstrated in the previous chapter, it is fairly easy to create a default linear network with the toolbox.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c505bd-fdb2-479e-88d7-14dc7f3fb0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the simplest version of a linear neural network this is all you have to do:\n",
    "model = SimpleLinearModel(784, 10)\n",
    "\n",
    "# we print the model summary here again.\n",
    "print(summary(model, (1, 784)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45a43e2-1d15-4cbc-a5b9-014aa8886bda",
   "metadata": {},
   "source": [
    "<p align=\"justify\">Now that we know a little bit about the data and we defined our network it is now time to train the network.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8e8b01-e01c-4f37-8b7f-c513fe42a66e",
   "metadata": {},
   "source": [
    "## Training the model to classify hand-written digits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72a4ed2-7535-41cd-96fc-777c437c966c",
   "metadata": {},
   "source": [
    "As the goal of the toolbox is to make it as easy as possible for you, the user, it comes with a predefined ```standard_train``` function. The code below (click the button to expand) shows you what this function looks like and, again, for ease of use, this function is being used by default when you create any model of the toolbox (e.g., ```SimpleLinearModel```, ```Simple2dCnnClassifier```, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2899f62e-6407-4e2d-80c1-ee859f2ee372",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "# the standard_train function\n",
    "# it requires some additional packages that we import here\n",
    "# in case you simply want to use it you do not have to worry about\n",
    "# implementing these packages, the toolbox does this automatically for you.\n",
    "# We just show it here for completeness\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def standard_train(\n",
    "        model,\n",
    "        train_data: DataLoader,\n",
    "        loss_fn=CrossEntropyLoss(),\n",
    "        optimizer=Adam, lr: float = .00001,\n",
    "        device: torch.device = torch.device(\"cpu\"),\n",
    "        train=True,\n",
    "        **optimizer_kwargs\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    r\"\"\"\n",
    "    A simple function to train a supplied neural network for multiclass problems.\n",
    "\n",
    "    Args:\n",
    "        model: only necessary if this function is not supplied to the model constructor. Otherwise model.fit()\n",
    "            calls the function with model=self\n",
    "        train_data (DataLoader): dataloader used for training or validation/test if function is called in model.eval()\n",
    "            context\n",
    "        loss_fn: loss function to use (default: CrossEntropyLoss)\n",
    "        optimizer: optimizer to use to adjust weights in backward pass (default: Adam)\n",
    "        lr (float): the learning rate for the optimizer\n",
    "        device (torch.device): do computations on device, e.g., cpu or gpu (default: cpu)\n",
    "        train (bool): set the network into training mode (True|default) or evaluation (False)\n",
    "        **optimizer_kwargs: additional arguments for the supplied optimizer\n",
    "\n",
    "    Returns:\n",
    "        Tuple[epoch_loss, stats]: the stats variable contains multiple values. The first 0:n_classes columns contains\n",
    "        the classification probability for a given input (row). The second to last column (i.e., stats[:,-2]) contains\n",
    "        the predicted label. The last column (i.e. stats[:, -1] contains the real label.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # set the model into training mode if it is not and train=True\n",
    "    if train:\n",
    "        if not model.training:\n",
    "            model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    # here is something new: the optimizer\n",
    "    # The optimizer determines the algorithm with which the weights of the layers\n",
    "    # are adjusted. Here we use the 'Adam' algorithm by default.\n",
    "    optimizer = optimizer(model.parameters(), lr=lr, **optimizer_kwargs)\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # the batch loop. Within this loop we iterate over all samples stored in the\n",
    "    # train_data variable.\n",
    "    # the variable 'batch' represents the current iteration [integer value]\n",
    "    # the variable 'inputs' is the actual input data in the shape of batch_size-by-inputshape\n",
    "    # the variable 'labels' contains the respective class label\n",
    "    for batch, (inputs, labels) in enumerate(train_data):\n",
    "\n",
    "        # in this function we transfer the data and the label tensors to the chosen device\n",
    "        # NOTE: I RECOMMEND TRANSFERING THE DATA AND LABELS TO THE DEVICE WHEN YOU LOAD THEM\n",
    "        # MORE ON THIS IN A DIFFERENT STEP THOUGH [see DataSets and DataLoaders)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # reset the gradients\n",
    "        # a crucial step in training the networks. Otherwise the gradients accumulate after\n",
    "        # each batch iteration and weird stuff will happen.\n",
    "        if model.training:\n",
    "            for p in model.parameters():\n",
    "                p.grad = None\n",
    "\n",
    "        outputs = model(inputs.float())  # forward pass through model\n",
    "\n",
    "        loss = loss_fn(outputs.squeeze(), labels.squeeze())  # calculate loss\n",
    "\n",
    "        # in this statement we check if the network is currently in training mode,\n",
    "        # which means, that every layer in the network has the required_grad flag set\n",
    "        # to True. This in turn means that the backpropagation algorithm is executed.\n",
    "        # If the model, however, is not in training mode we do not want to exectue\n",
    "        # the backward pass and we also do not want to store the so-called pytorch graph.\n",
    "        if model.training:\n",
    "            loss.backward()  # do a backward pass\n",
    "            optimizer.step()  # update parameters\n",
    "\n",
    "        # get the probabilities of the predictions\n",
    "        prediction_probs = model.SM(outputs.data).cpu().numpy()\n",
    "\n",
    "        # get the label number of the output\n",
    "        _, predicted_labels = torch.max(outputs.data, 1)\n",
    "\n",
    "        epoch_loss += loss.item()  # sum up the loss over all batches\n",
    "\n",
    "        # just a helper variable\n",
    "        inter = np.hstack(\n",
    "            [prediction_probs.squeeze(),\n",
    "             labels.squeeze().cpu().numpy()[:, None],\n",
    "             predicted_labels.squeeze().cpu().numpy()[:, None]]\n",
    "        )\n",
    "        stats = np.vstack([stats, inter]) if 'stats' in vars() else inter  # noqa\n",
    "\n",
    "    return epoch_loss / len(train_data), stats  # noqa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e90624-0941-4df2-bfb7-f62fed1ce9f9",
   "metadata": {},
   "source": [
    "Okay. Now that we know a little bit about how our model is trained, we need to do some preparation work first.\n",
    "That means we will download the MNIST dataset and then setup a training loop.\n",
    "\n",
    "Let's do that!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4a44fa-7362-4d7d-aa91-813025413103",
   "metadata": {},
   "source": [
    "### Define the training procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e02b836-247e-40ed-88d1-dc4163214255",
   "metadata": {},
   "source": [
    "We should first make sure that our network is also on our detected device. In case you do not send your model or your data to the same device you will run into errors. \n",
    "\n",
    "Next, we should determine for how long, that is how many *epochs*, we want to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614749c4-95da-44ae-a1f1-7bcea5783318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we should make sure the network is on the correct device:\n",
    "model.to(DEVICE);\n",
    "\n",
    "# define the number of epochs (that is training iterations) you want your model to go through\n",
    "# let's say we want this to be 10 epochs\n",
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74930b45-9cfc-4774-afa2-b520f1a0e3f1",
   "metadata": {},
   "source": [
    "```{warning}\n",
    "Make sure you only send your model to a device *once*. If you have multiple lines of this model.to(DEVICE) it can lead to unwanted results. I speak from experience\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8382cb56-ea11-4e0f-ba65-fd2ae0a3cc33",
   "metadata": {},
   "source": [
    "Now we setup the dataloaders for the training function. In case you have not gone through the tutorial on {ref}`dataloader <dataloader_chapter>` I suggest you do this at some point to undestand what they are good for and how to define them.\n",
    "\n",
    "The PyTorch community has a really good introduction on their [offcial website](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f48c35-9afe-4008-92c9-e49eb1ea9d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataloaders\n",
    "dl_train = DataLoader(mnist_train, batch_size=256, shuffle=True, generator=g)\n",
    "dl_test = DataLoader(mnist_test, batch_size=256, shuffle=True, generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d402b531-d913-4c77-ae2e-71b873c91825",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "# start training the network\n",
    "\n",
    "# set some variables here, such that we can create pretty plots\n",
    "train_loss = np.zeros(n_epochs)\n",
    "test_loss = np.zeros_like(train_loss)\n",
    "train_acc = np.zeros_like(train_loss)\n",
    "test_acc = np.zeros_like(train_loss)\n",
    "\n",
    "# loop for the above set number of epochs\n",
    "for epoch in range(0, n_epochs):\n",
    "\n",
    "    # THIS IS WHERE THE MAGIC HAPPENS\n",
    "    # calling the model.fit() function will execute the 'standard_train' function as defined above.\n",
    "    train_loss[epoch], train_stats = model.fit(dl_train, lr=.001, device=DEVICE)\n",
    "    train_acc[epoch] = compute_accuracy(train_stats[:, -1], train_stats[:, -2])\n",
    "\n",
    "    # for validating or testing set the network into evaluation mode such that layers like dropout are not active\n",
    "    with torch.no_grad():\n",
    "        test_loss[epoch], test_stats = model.fit(dl_test, device=DEVICE, train=False)\n",
    "        test_acc[epoch] = compute_accuracy(test_stats[:, -1], test_stats[:, -2])\n",
    "\n",
    "    print('epoch=%03d, train_loss=%1.3f, train_acc=%1.3f, test_loss=%1.3f, test_acc=%1.3f' % \n",
    "         (epoch, train_loss[epoch], train_acc[epoch], test_loss[epoch], test_acc[epoch]))\n",
    "\n",
    "# let's save the model\n",
    "model.save('my_lin_mnist', save_full=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667aa442-b9a6-4bce-bf81-adb9a0534c3a",
   "metadata": {},
   "source": [
    "That was not so bad was it? Quite simple in fact, I hope. <br>\n",
    "Please keep in mind that we try to incorporate the epoch loop into the model.fit() function as well. <br>For now, we unfortunately encounter some memory leaks that we first need to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639b3658-ce0e-43fd-a4da-ab2c7d80c2c1",
   "metadata": {},
   "source": [
    "### Check model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7793ce5-b1bf-4c88-b033-12f79f12e37b",
   "metadata": {},
   "source": [
    "Let's look if our network really learned something. For that we will plot some loss and accuracy scores as a function of the epoch as well as a confusion matrix for the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0b7839-6bef-45e8-96a2-4faf2de0a318",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "from _core.utils.plots import confusion_matrix\n",
    "\n",
    "fig, axes = plt.subplots(1,3,figsize=(16,6))\n",
    "axes[0].plot(range(1,n_epochs+1), train_loss, range(1,n_epochs+1), test_loss, linewidth=3, marker=\"o\", markersize=10, markeredgecolor=\"white\")\n",
    "axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('CrossEntropyLoss'), axes[0].spines['top'].set_visible(False), axes[0].spines['right'].set_visible(False);\n",
    "axes[0].legend((\"train\", \"test\"));\n",
    "\n",
    "axes[1].plot(range(1,n_epochs+1), train_acc, range(1,n_epochs+1), test_acc, linewidth=3, marker=\"o\", markersize=10, markeredgecolor=\"white\")\n",
    "axes[1].axhline(.1, color='black', linestyle='--')\n",
    "axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('Accuracy'), axes[1].spines['top'].set_visible(False), axes[1].spines['right'].set_visible(False);\n",
    "axes[1].legend((\"train\", \"test\", \"chance\"));\n",
    "\n",
    "lin_conf_mat = confusion_matrix(test_stats[:,-2], test_stats[:,-1], range(0,10), ax=axes[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df19e221-64fd-4d00-a9af-da030a025c38",
   "metadata": {},
   "source": [
    "This looks great! The line plots clearly indicate a downard trend in the loss curve (left) and an upward trend in the accuracy curve, respectively. Further, we can see that the test loss is lower than in the train. This is somewhat interesting as usually the training set performs better than the test or validation data. It is important to note that the loss and accuracy scores are averages for a given epoch. The training set is about 6 times larger than the test set in the case of MNIST. This is only one explanation of why we get curves like these.\n",
    "\n",
    "Now, let me show you how easy it is to switch to another type of neural network: a convolutional neural network (CNN) model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4446ca3-8ae6-4dac-85b1-de1371a40209",
   "metadata": {},
   "source": [
    "## Using a CCN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ddd474-18fa-4bf3-ac64-b0fb634c0ba9",
   "metadata": {},
   "source": [
    "Changing to a CNN with the toolbox is quite simple. Instead of the ```SimpleLinearModel``` we now use the ```Simple2dCnnClassifier```.\n",
    "\n",
    "As the name suggest we are now working with data that is 2-dimensional. Luckily for us the MNIST images are just that. We also know that the MNIST images have 28-by-28 pixels. Thus our input_dims for the model are now [28, 28]. The number of classes remains at 10.\n",
    "\n",
    "We also again can use the ```summary``` function to better understand how the model is configured. \n",
    "\n",
    "```{note}\n",
    "Images usually have color channels, thus the input of our images needs to be (batch, channels, pixeldim1, pixeldim2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d049e793-b3a8-49f0-b2fa-5970da0b07e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we actually do exactly the same as for the linear model. \n",
    "model = Simple2dCnnClassifier((28, 28), 10)\n",
    "print(summary(model, (1, 1, 28, 28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dd6080-5725-4567-9681-e3261da9667a",
   "metadata": {},
   "source": [
    "Alrighty. Make sure you send the model to the device. We can keep the number of epochs and the dataloader the same as for the ```SimpleLinearModel```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff41333-6e84-4df5-b632-c71f520fd986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we should make sure the network is on the correct device:\n",
    "model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22daa2ec-a594-42f1-bdbe-b4a1b371e8a3",
   "metadata": {},
   "source": [
    "Ok! Time to train your first 2-D CNN!\n",
    "    \n",
    "Notice that the code below essentially is the same as it is for the linear model. The only thing that changed is some variable names such that we can compare them. But isn't that great? If we find a nice way we turn the below code into a function and simply supply a model to it and it runs. No matter the network.\n",
    "\n",
    "```{note}\n",
    "I also changed the learning rate from .001 to .01. Interesting effects can happen if one decreases or increases the learning rate. It is quite common however, that for deeper CNNs larger learning rates work better. \n",
    "```\n",
    "\n",
    "Finding the best, or at least a good combination of parameters is what we cover in the {ref}`next chapter <using_sweeps>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d1e76d-ec33-480e-b4d8-9eb467ceb6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training the network\n",
    "\n",
    "# set some variables here, such that we can create pretty plots\n",
    "cnn_train_loss = np.zeros(n_epochs)\n",
    "cnn_test_loss = np.zeros_like(train_loss)\n",
    "cnn_train_acc = np.zeros_like(train_loss)\n",
    "cnn_test_acc = np.zeros_like(train_loss)\n",
    "\n",
    "# loop for the above set number of epochs\n",
    "for epoch in range(0, n_epochs):\n",
    "\n",
    "    # THIS IS WHERE THE MAGIC HAPPENS\n",
    "    # calling the model.fit() function will execute the 'standard_train' function as defined above.\n",
    "    cnn_train_loss[epoch], cnn_train_stats = model.fit(dl_train, lr=.01, device=DEVICE)\n",
    "    cnn_train_acc[epoch] = compute_accuracy(cnn_train_stats[:, -1], cnn_train_stats[:, -2])\n",
    "\n",
    "    # for validating or testing set the network into evaluation mode such that layers like dropout are not active\n",
    "    with torch.no_grad():\n",
    "        cnn_test_loss[epoch], cnn_test_stats = model.fit(dl_test, device=DEVICE, train=False)\n",
    "        cnn_test_acc[epoch] = compute_accuracy(cnn_test_stats[:, -1], cnn_test_stats[:, -2])\n",
    "\n",
    "    print('epoch=%03d, train_loss=%1.3f, train_acc=%1.3f, test_loss=%1.3f, test_acc=%1.3f' % \n",
    "         (epoch, cnn_train_loss[epoch], cnn_train_acc[epoch], cnn_test_loss[epoch], cnn_test_acc[epoch]))\n",
    "\n",
    "# Let's save the model\n",
    "model.save('my_cnn_mnist', save_full=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc2aab4-e2ca-44a0-a5d7-bce5383ee5e1",
   "metadata": {},
   "source": [
    "Very nice. Let us see how the CNN performes compared to the linear network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890528e7-67a5-40b7-aee7-a84e5baaa9d5",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "from _core.utils.plots import confusion_matrix\n",
    "\n",
    "fig, axes = plt.subplots(1,3,figsize=(16,6))\n",
    "axes[0].plot(range(1,n_epochs+1), train_loss, \n",
    "             range(1,n_epochs+1), test_loss, linewidth=3, marker=\"o\", markersize=10, markeredgecolor=\"white\")\n",
    "axes[0].plot(range(1,n_epochs+1), cnn_train_loss,\n",
    "             range(1,n_epochs+1), cnn_test_loss, linewidth=3, linestyle=\":\", marker=\"o\", markersize=10, markeredgecolor=\"white\")\n",
    "axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('CrossEntropyLoss'), axes[0].spines['top'].set_visible(False), axes[0].spines['right'].set_visible(False);\n",
    "axes[0].legend((\"lintrain\", \"lintest\", \"cnntrain\", \"cnntest\"));\n",
    "\n",
    "axes[1].plot(range(1,n_epochs+1), train_acc, \n",
    "             range(1,n_epochs+1), test_acc, linewidth=3, marker=\"o\", markersize=10, markeredgecolor=\"white\")\n",
    "axes[1].plot(range(1,n_epochs+1), cnn_train_acc,\n",
    "             range(1,n_epochs+1), cnn_test_acc, linewidth=3, linestyle=\":\", marker=\"o\", markersize=10, markeredgecolor=\"white\")\n",
    "axes[1].axhline(.1, color='black', linestyle='--')\n",
    "axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('Accuracy'), axes[1].spines['top'].set_visible(False), axes[1].spines['right'].set_visible(False);\n",
    "axes[1].legend((\"lintrain\", \"lintest\", \"cnntrain\", \"cnntest\", \"chance\"));\n",
    "\n",
    "cnn_conf_mat = confusion_matrix(cnn_test_stats[:,-2], cnn_test_stats[:,-1], range(0,10), ax=axes[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ece49b-1979-46f1-8a50-33d74fdfc672",
   "metadata": {},
   "source": [
    "<p align=\"justify\">It seems that the default CNN outperforms the default linear network. Pretty cool. However, we can also see that at about epoch 9 the train loss for the CNN seems to increase again. This might already indicate some overfitting issues. Something to bear in mind! \n",
    "\n",
    "The graph below shows the difference of the CNN confusion matrix minus the LN confusion matrix. We can immediately see that the CNN outperforms the LN because the diagonal clearly has positive values in the majority of the classes. Remember: the diagonal means that the network correctly predicts the real class.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e48848b-491b-47a6-98aa-ab45e18c66d1",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(12,6))\n",
    "a = ax.imshow(cnn_conf_mat-lin_conf_mat, cmap='seismic');\n",
    "plt.colorbar(a);\n",
    "ax.set_title('Diff CNN_conf - Lin_conf');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d7d89e-1548-46c6-9b49-6cd6a8abb3da",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c14b771-c12f-4091-85f7-b3ea92235724",
   "metadata": {},
   "source": [
    "* Try using the default (or your own configuration) of the ```SimpleLinearModel``` and the ```Simple2dCnnClassifier``` on the Fashion-MNIST dataset. See which performs better!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
