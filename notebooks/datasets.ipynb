{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c642d917-7d98-4071-9d1a-a906b136539c",
   "metadata": {
    "tags": []
   },
   "source": [
    "(dataloader_chapter)=\n",
    "# PyTorch datasets and dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66a8583-9d4d-43ef-a1be-51aa0b9a2ee8",
   "metadata": {},
   "source": [
    "The PyTorch [dataset and dataloader](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) classes make it easy for us to use batching etc. Their tutorial is a good place to start understanding how they work and should be read alongside this tutorial here in case things are unclear.\n",
    "\n",
    "You can use them by importing the ```DataLoader``` and ```Dataset``` packages from ```torch.utils.data```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8de1583-79cc-48a7-bd50-1cac2b552e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from delphi.utils.tools import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0a65ca-fe79-43ce-b387-3acdefd31ce0",
   "metadata": {},
   "source": [
    "Unfortunately, I cannot provide Dataset classes for basically any usecase unless everyone follows the conventions that we follow in the Biomedical Imaging Group. However, I do propose that you employ a data tree like the one shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787250f6-24b8-4d7b-a73e-c88e63ad71df",
   "metadata": {},
   "source": [
    "```{image} ../_images/datastruct.png\n",
    ":alt: example datastructure\n",
    ":width: 150px\n",
    ":align: center\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2b313b-23a2-4b87-a8b5-1772646fa842",
   "metadata": {},
   "source": [
    "This helps us in quickly identifying the label of a given class just by looking up the directory name. For some use cases, for example with tabular data (such as .csv or .xls datatypes) the labels usually are assigned in one column of the data. Then a directory structure like it is shown above is unecessary.\n",
    "\n",
    "Let us now see what we need to implement for our custom dataset classes. The code block below depicts a template of the functions you absolutely need to implement when designing your own dataset class. To better understand what each of the functions do read the comments I provide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3030417-13b3-4f7e-824d-3e9d11e1f283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the general setup of a Dataset class\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        r\"\"\"\n",
    "        This is the so-called constructor of any class. As the name suggests\n",
    "        this function is used to initialize important variables that the dataset\n",
    "        needs to know. This construct function is automatically called when \n",
    "        you instantiate a new class, e.g., when you call data = MyDataset()\n",
    "        This function can also have arguments. I suggest that the constructor\n",
    "        should have at least these parameters: \n",
    "        \n",
    "        Args:\n",
    "            path_to_data (str): this could be either a path to mulitple files\n",
    "            or the file itself.\n",
    "            \n",
    "            device (torch.device): which device would you like to use \n",
    "            (default=torch.device(\"cpu\"))\n",
    "            \n",
    "            shuffle_labels (bool): this is a nice addition in case one wants\n",
    "            to create a null performance measure (default: False)\n",
    "            \n",
    "            transform: in case one wants to transform the data (i.e., normalize,\n",
    "            rotate, scale, etc.)\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        r\"\"\"\n",
    "        returns the length of the dataset\n",
    "        this is done by taking the len() of the dataset\n",
    "        \n",
    "        return len(self.data)\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        r\"\"\"\n",
    "        This is the function in which we actually load the data and the labels.\n",
    "        In here it is common to set up the data such that you return a tuple of\n",
    "        the data and the labels as their own variables. In this function we also\n",
    "        move the data and labels to the supplied device and transform them according\n",
    "        to the supplied transformation functions in self.transform (if they exist).\n",
    "        \n",
    "        Args:\n",
    "            idx (int): indicates with datum and label to return.\n",
    "            \n",
    "        Returns:\n",
    "            tuple(data, label)\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13b5225-9b6b-4673-b278-81aff042f8ba",
   "metadata": {},
   "source": [
    "Ok, now that we have an intuition of what a dataset class looks like let us look at some implementations that I provide with my code.\n",
    "\n",
    "The first implementation we look at is a ```TabularDataset``` class. This dataset can take a ```.csv``` or an excel compatible (e.g., ```.xls, .xlsx, .odf, .ods``` among others) file-type as input. \n",
    "\n",
    "```{note}\n",
    "Do not be discouraged by the extra code that is in this implementation. This is to prevent erronous inputs etc. and is meant to guide the user. In most cases this is not necessary because you will commonly write code suited to your use case. I tried to be as general as possible.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0942ea83-4eef-41ff-891b-b8f24687bbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us create a simple dataset that takes .csv or maybe other tabular files as input\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# we call this class TabularDataset since that is what it is\n",
    "class TabularDataset(Dataset): \n",
    "    r\"\"\"\n",
    "    Class to take care of tabular files such as .csv, .xls, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    # we can support different types of tabular dataformats.\n",
    "    # for now we support the file types given in the variables below.\n",
    "    POSSIBLE_EXCEL_EXTENSIONS = [\".xls\", \".xlsx\", \".xlsm\", \".xlsb\", \".odf\", \".ods\", \".odt\"]\n",
    "    \n",
    "    # in this implementation it is required that at one column has one of the following \n",
    "    # descriptions. If it does not exist in the data raise an error.\n",
    "    EXPECTED_LABEL_COLUMN_NAMES = [\"class\", \"label\", \"target\"]\n",
    "    \n",
    "    ####### REQUIRED CLASS FUNCTIONS ########\n",
    "    \n",
    "    # now we come to the so-called constructor or the initializer function\n",
    "    # I personally like having the option to add transformation functions and \n",
    "    # the option to shuffle the labels. This allows me to quickly create\n",
    "    # a null distribution/performance estimate\n",
    "    def __init__(\n",
    "        self, \n",
    "        path_to_file, \n",
    "        device=torch.device(\"cpu\"), \n",
    "        shuffle_labels=False, \n",
    "        transform=None\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        The constructor of the TabularDataset class.\n",
    "        \n",
    "        Args:\n",
    "            path_to_file (str): the path to the file. Supports .csv, .xls file-types at the moment\n",
    "            device (torch.device): the device on which to store the data\n",
    "            shuffle_labels (bool): Default=False; permutes the class labels\n",
    "            transform: can be a list of functions to transform the data\n",
    "        \"\"\"     \n",
    "        super(TabularDataset, self).__init__()\n",
    "        \n",
    "        self.path_to_file   = path_to_file\n",
    "        self.shuffle_labels = shuffle_labels\n",
    "        self.transform      = transform\n",
    "        self.device         = device\n",
    "        \n",
    "        # we can check what the file extension of the supplied file is. \n",
    "        # this informs us which function to use to read the file.\n",
    "        filename, file_extension = os.path.splitext(self.path_to_file)\n",
    "        \n",
    "        # read the file 'path_to_file' with pandas reading functions\n",
    "        if file_extension == '.csv':\n",
    "            self.data = pd.read_csv(self.path_to_file)\n",
    "            \n",
    "        elif file_extension in self.POSSIBLE_EXCEL_EXTENSIONS:\n",
    "            self.data = pd.read_excel(self.path_to_file)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"{file_extension} is not \\'.csv\\' or one of {self.POSSIBLE_EXCEL_EXTENSIONS}\")\n",
    "        \n",
    "        # check if a [\"class\", \"label\", \"target\"] column is found in the data\n",
    "        # if not, raise an error\n",
    "        self.label_column = self._check_for_label_column()\n",
    "        \n",
    "    def __len__(self):\n",
    "        r\"\"\"\n",
    "        returns the length, i.e. the number of samples, of the dataset\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        r\"\"\"\n",
    "        get the (batch) sample and label. A sample is one row of the dataset.\n",
    "        A column represents one feature. \n",
    "        \n",
    "        Returns:\n",
    "            tuple(sample, label)\n",
    "        \"\"\"\n",
    "        # select all the data except for the label column\n",
    "        data = self.data.loc[:, self.data.columns != self.label_column].to_numpy()\n",
    "        \n",
    "        # extract only the requested row\n",
    "        sample = data[idx, :]\n",
    "        \n",
    "        # assign the corresponding label to the row\n",
    "        label = self.data[self.label_column].to_numpy()[idx]\n",
    "        \n",
    "        # In case you provide a set of transformations execute them here\n",
    "        if self.transform:\n",
    "            label = self.transform(label).to(self.device)\n",
    "            sample = self.transform(sample).float().to(self.device)\n",
    "        \n",
    "        return (sample, label)\n",
    "    \n",
    "    ####### CUSTOM / HELPER FUNCTIONS ########\n",
    "    \n",
    "    def _check_for_label_column(self):\n",
    "        r\"\"\"\n",
    "        make sure the dataset has a column indicating the label, class, or target\n",
    "        \n",
    "        Returns:\n",
    "            label_column: the column name of the target/class/label\n",
    "        \"\"\"\n",
    "        # make sure all column values are lowercase\n",
    "        columns = [column_name.lower() for column_name in self.data.columns.to_list()]\n",
    "        \n",
    "        # check if there is a column with one of these descriptions: [\"class\", \"label\", \"target\"]\n",
    "        # if not, raise an error and indicate to the user that they need a label column with\n",
    "        # the description in EXPECTED_LABEL_COLUMN_NAMES\n",
    "        label_column = [col_name for col_name in self.EXPECTED_LABEL_COLUMN_NAMES if columns.count(col_name) > 0]\n",
    "        \n",
    "        if not label_column:\n",
    "            raise ValueError(f\"Did not find a column indicating the {self.EXPECTED_LABEL_COLUMN_NAMES}\")\n",
    "        \n",
    "        return label_column[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c57658-ca07-4016-9d63-f25f79469082",
   "metadata": {},
   "source": [
    "Right, so what was all this for? The goal of these datasets is to have a general setup such that we can exploit the power of the PyTorch ```DataLoader``` class.\n",
    "\n",
    "The ```DataLoader``` class can take care of a number of things for us. \n",
    "* It is a ```generator``` which means we can easily iterate over all datapoints in our dataset\n",
    "* we can set how many samples per batch we want\n",
    "* we can shuffle the dataset\n",
    "* we can distribute the data/batches to multiple workers, meaning we can exploit parallel processing\n",
    "* etc...\n",
    "\n",
    "I created a ```dummy_ds.csv``` file to demonstrate how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55101d0b-ddb5-4d0d-9193-08e0869608d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now load the data stored within the dummy_ds.csv file\n",
    "data = TabularDataset('dummy_ds.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9809df02-b1d3-4019-bb19-9dba90968008",
   "metadata": {},
   "source": [
    "If we wanted to, we could now access each sample by using the ```__getitem__``` function and supply a single index as shown below. On the other hand if we wanted to get a batch, we could also supply a list of indices to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cb9931d-e2c8-4ba6-85b1-976638588452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data:  [ 0.34810022  1.10560502 -0.12978938  1.68536215 -0.01200362  2.12168985\n",
      " -0.53985801 -0.45726264 -0.57580312  0.96060891 -0.46569884  0.94239489\n",
      "  1.63580326  0.39518677 -0.49509537 -0.4432387   1.01888881 -0.68882815\n",
      "  0.05426619  0.03923666] \n",
      "Label:  0\n",
      "\n",
      "Batched Sample data:  [[-1.25744765  0.99514179  0.76161641  1.27121089 -0.1042117  -0.42573839\n",
      "  -0.66558342  0.88723275  0.1171774  -0.87590716  1.39366192  0.75422015\n",
      "  -1.61686852  0.22071409 -0.89460092  0.63354489  1.32051097 -0.23074283\n",
      "   0.99393088  0.35410665]\n",
      " [ 0.96989177  0.28143284 -0.35807049  1.37952275  0.07642126  0.26766915\n",
      "  -0.86966526  0.64460088 -0.33722325 -0.35106016  1.05169211  0.79733317\n",
      "  -1.98811722 -0.52962523  0.71545117  0.21856745  1.52769592  0.35716858\n",
      "  -0.29743858  2.74152369]\n",
      " [ 0.62496938 -0.55165461  0.45645439  1.03613864  1.09365959 -2.26105163\n",
      "   1.64820326  1.95124017 -0.03287803 -2.18782345 -0.59050062  1.41066745\n",
      "  -0.93724356  0.01958021 -0.68527278  1.25714178  0.05581559  0.85175372\n",
      "  -1.33724658 -0.26182352]\n",
      " [ 0.34810022  1.10560502 -0.12978938  1.68536215 -0.01200362  2.12168985\n",
      "  -0.53985801 -0.45726264 -0.57580312  0.96060891 -0.46569884  0.94239489\n",
      "   1.63580326  0.39518677 -0.49509537 -0.4432387   1.01888881 -0.68882815\n",
      "   0.05426619  0.03923666]] \n",
      "Batched Labels:  [1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# get a single sample with its label\n",
    "sample, label = data.__getitem__(10)\n",
    "print(\"Sample data: \", sample, \"\\nLabel: \", label)\n",
    "\n",
    "# get a batch of 4 samples with their respective labels\n",
    "sample, label = data.__getitem__([2, 4, 0, 10])\n",
    "print(\"\\nBatched Sample data: \", sample, \"\\nBatched Labels: \", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36b8271-eb9f-40c6-8892-f013e7a49e80",
   "metadata": {},
   "source": [
    "The thing is, we are lazy. We do not want to bother doing this ourselves, so we use the ```DataLoader``` class I mentioned above. This time, we will use a loop to see how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed1bf136-b48f-4c9a-ab3d-c1ac94794639",
   "metadata": {
    "scrolled": true,
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 10\n",
      "\n",
      "Batched Sample data:  tensor([[-0.8591, -1.7704, -0.1745, -0.9754,  0.5658,  0.1900, -0.7620, -0.7420,\n",
      "          1.3585,  0.1110, -0.3965,  0.1332,  1.8672, -0.0978, -1.0952,  0.0330,\n",
      "          1.8324,  0.0407, -1.0200, -0.5890],\n",
      "        [ 0.1291, -1.7352,  0.7939, -1.7178,  0.6857, -0.9208, -0.3240, -1.6038,\n",
      "          0.1892, -1.4704, -0.3705, -0.1547,  0.9224,  0.0558,  0.6375,  0.5315,\n",
      "          1.4825,  0.5101, -1.3424,  1.1783],\n",
      "        [-0.2652, -0.4826,  0.3204,  0.7555, -0.6018,  0.1626,  0.2298,  1.1416,\n",
      "          1.0210,  1.1791, -0.6822, -0.4467, -0.7880, -0.5294, -0.6582,  0.6906,\n",
      "          0.6552,  1.6260, -0.5466,  0.4498],\n",
      "        [ 0.2328,  1.6465,  1.4599,  1.9211,  0.5543,  0.8886, -1.5124,  0.9497,\n",
      "          0.0959,  2.7158, -0.9761,  0.5828, -0.0260, -0.2237, -0.1546,  1.9794,\n",
      "          0.5556,  0.0892, -0.1464,  0.1042]], dtype=torch.float64) \n",
      "Batched Labels:  tensor([0, 0, 1, 0]) \n",
      "\n",
      "Batched Sample data:  tensor([[-4.5431e-01, -6.5284e-01,  5.4846e-01, -5.8191e-01, -8.0843e-01,\n",
      "          1.1568e+00,  1.6788e+00,  3.6142e-01, -1.0716e+00,  6.8956e-01,\n",
      "         -8.3510e-03, -5.7715e-01,  6.7672e-01,  1.6095e+00,  1.4942e+00,\n",
      "         -2.6420e-01,  1.4861e+00, -1.2386e-01, -1.0083e+00,  1.0547e-03],\n",
      "        [ 3.7356e-01, -5.3909e-01,  4.7391e-01,  2.3527e+00,  6.9724e-01,\n",
      "          3.5875e-01,  1.5851e+00,  1.7224e-01,  8.2186e-03, -1.5335e+00,\n",
      "         -1.4860e+00,  1.8460e+00, -8.7445e-03,  6.4874e-01,  6.6622e-01,\n",
      "          1.9224e+00, -2.6458e-01,  5.9561e-01, -2.4767e-01,  9.5394e-01],\n",
      "        [-1.3296e-01, -1.8377e+00, -4.5377e-01, -7.0391e-01,  1.0572e+00,\n",
      "          2.9513e-01,  7.2276e-01,  1.6196e-01,  1.9225e-01, -1.4306e+00,\n",
      "          3.7392e-01,  1.1288e+00,  1.0969e+00,  2.6004e-01, -1.6272e+00,\n",
      "         -8.7144e-01, -6.5671e-01,  1.4668e+00, -8.0264e-02,  1.1069e+00],\n",
      "        [-2.5682e+00, -1.4403e-01, -1.0137e+00,  1.0937e+00,  9.7022e-02,\n",
      "         -9.8515e-01,  6.5434e-01, -1.7555e+00, -1.0792e+00, -5.4460e-01,\n",
      "          3.5445e-01,  6.7066e-01, -1.6439e+00, -4.2588e-01,  4.9385e-01,\n",
      "          3.1124e-01,  6.3132e-01,  2.1230e+00,  4.1882e-01, -1.2571e+00]],\n",
      "       dtype=torch.float64) \n",
      "Batched Labels:  tensor([0, 0, 0, 0]) \n",
      "\n",
      "Batched Sample data:  tensor([[ 0.2644,  0.0792,  0.1407,  0.4541, -0.2090,  0.7907, -0.3052, -0.1008,\n",
      "          0.5944, -0.2572,  1.2908, -0.4554,  0.3059, -1.8553,  0.4927, -1.5877,\n",
      "          0.4509,  0.9855,  0.2597, -0.0996],\n",
      "        [ 1.1794,  1.3873,  0.7549, -1.1792, -1.1329,  0.4552, -0.2438,  0.2792,\n",
      "         -0.1336, -0.7018, -0.3132, -1.2734,  0.6218,  0.1401, -1.2727, -0.7305,\n",
      "          2.2915, -0.7038, -0.8197,  0.8068],\n",
      "        [ 1.2251,  2.1953, -0.8511,  0.5282,  1.5062, -0.7324,  0.2272,  0.4934,\n",
      "         -1.2742,  0.8370, -0.2388,  1.6977, -1.2315,  0.6933,  1.9371,  1.6288,\n",
      "         -0.7499,  0.8814, -0.7778, -1.0388],\n",
      "        [ 1.3053,  0.6896, -0.9933, -1.3382, -0.1248,  0.2594, -0.4363,  1.2170,\n",
      "         -0.6823,  0.2864, -0.3140,  0.5833,  1.1667,  0.7672, -0.7090, -0.5304,\n",
      "          0.8625, -0.2286, -0.3526,  0.3228]], dtype=torch.float64) \n",
      "Batched Labels:  tensor([1, 1, 0, 0]) \n",
      "\n",
      "Batched Sample data:  tensor([[ 2.0860, -0.8513, -0.3500, -1.0896, -1.0691, -0.3002, -0.9221, -0.3040,\n",
      "         -0.0644,  0.2230,  0.2455,  1.3858,  0.5839, -0.5200,  1.1862, -1.4604,\n",
      "          1.7749, -0.0087,  0.4553, -0.4010],\n",
      "        [-0.6855,  1.2703, -0.1051,  0.2555, -1.3376,  0.4905, -0.2704,  0.3152,\n",
      "         -1.0613, -0.2843,  0.4255,  0.9754, -1.2756, -1.2510, -0.0551,  0.3331,\n",
      "         -1.7431,  0.7307,  0.5624,  1.2947],\n",
      "        [-0.1633,  0.1996,  0.2825, -0.5784, -0.9747,  1.4508,  1.0442, -0.1510,\n",
      "         -0.2763,  0.0249, -1.1026, -0.2910, -1.2027,  0.3704, -0.0967,  0.5378,\n",
      "         -0.2882,  1.1478,  0.3078, -0.5213],\n",
      "        [-0.8122, -2.2237,  1.8629,  0.3543, -0.8057,  0.1588,  1.5893,  0.2089,\n",
      "         -0.3596,  0.6183, -0.9610,  0.3633, -1.0089,  0.9683,  0.4022, -0.5878,\n",
      "          1.6318, -1.1241,  0.3019,  0.2625]], dtype=torch.float64) \n",
      "Batched Labels:  tensor([1, 0, 0, 1]) \n",
      "\n",
      "Batched Sample data:  tensor([[-1.7191e-01,  1.6968e-01, -1.2872e+00, -4.8508e-01,  1.7710e+00,\n",
      "          6.4900e-01, -8.0424e-01, -1.1109e+00,  2.0882e-01, -1.0708e+00,\n",
      "         -1.8083e+00,  1.4519e+00, -1.2646e-01,  5.6125e-01, -4.7167e-01,\n",
      "         -7.8203e-01,  1.1221e-01,  1.1825e+00, -6.0758e-01, -9.9704e-01],\n",
      "        [-1.2574e+00,  9.9514e-01,  7.6162e-01,  1.2712e+00, -1.0421e-01,\n",
      "         -4.2574e-01, -6.6558e-01,  8.8723e-01,  1.1718e-01, -8.7591e-01,\n",
      "          1.3937e+00,  7.5422e-01, -1.6169e+00,  2.2071e-01, -8.9460e-01,\n",
      "          6.3354e-01,  1.3205e+00, -2.3074e-01,  9.9393e-01,  3.5411e-01],\n",
      "        [-6.0014e-01, -4.1300e-01,  8.7484e-01, -8.6698e-01,  7.7486e-01,\n",
      "         -8.1261e-01, -7.6997e-01,  8.6132e-01, -3.7165e-01,  9.9648e-01,\n",
      "         -6.2527e-01, -5.8556e-01, -1.7908e+00, -1.5097e-04,  6.3818e-01,\n",
      "         -1.0593e+00,  5.0057e-01, -1.8029e+00,  8.0016e-01, -4.2365e-01],\n",
      "        [ 1.6549e+00, -7.2979e-01, -1.9167e+00, -1.3042e+00, -4.1387e-01,\n",
      "          1.8389e+00,  9.2987e-01,  3.6860e-01,  7.0635e-01,  7.3465e-01,\n",
      "          3.6509e-01,  1.3826e+00,  5.0869e-01,  9.4376e-01, -2.2352e-01,\n",
      "         -6.3710e-01, -2.3176e-01,  6.2233e-01,  3.9981e-01, -2.2485e-02]],\n",
      "       dtype=torch.float64) \n",
      "Batched Labels:  tensor([1, 1, 1, 1]) \n",
      "\n",
      "Batched Sample data:  tensor([[-0.9908, -0.8721,  0.9936, -3.0987, -0.8415, -0.9533, -1.1682,  1.2055,\n",
      "          0.1890,  0.6690, -1.0472, -0.6849, -1.1405, -0.3104,  0.2768,  1.4617,\n",
      "          1.9639,  1.8423,  1.1516,  0.0090],\n",
      "        [-0.3102, -1.0169, -0.9181,  0.4909,  0.3858,  0.1634,  0.4832,  0.0442,\n",
      "         -1.7266, -0.7838,  0.9721, -0.4349,  1.4985,  0.6380, -1.3660,  0.1064,\n",
      "         -0.6543,  0.5634, -1.3375, -1.5044],\n",
      "        [ 0.2015, -0.4957,  1.0547, -0.9296, -1.4528,  0.2649,  0.4844,  1.4317,\n",
      "         -1.0968,  1.5127, -0.2845, -0.5777,  0.1554,  1.9389,  0.5942, -2.0412,\n",
      "         -0.6742, -1.2906, -0.9375, -1.6318],\n",
      "        [-0.1545, -1.1284,  0.2441,  0.7381, -1.0618,  1.0538, -0.6163, -1.1974,\n",
      "          0.4761,  1.4900, -0.6174,  1.2542, -0.9428,  1.1081, -2.1357,  1.8701,\n",
      "          0.2824,  0.4345, -0.3151,  0.4735]], dtype=torch.float64) \n",
      "Batched Labels:  tensor([1, 0, 1, 1]) \n",
      "\n",
      "Batched Sample data:  tensor([[-0.4301,  0.1053,  0.1697, -0.6605, -0.0667,  0.7893,  1.6357,  2.3572,\n",
      "          0.5476, -0.1736, -0.6049, -0.7453,  2.3920,  0.1429, -0.7036, -0.2870,\n",
      "         -1.4799, -3.0451,  0.3138,  1.3036],\n",
      "        [ 0.6250, -0.5517,  0.4565,  1.0361,  1.0937, -2.2611,  1.6482,  1.9512,\n",
      "         -0.0329, -2.1878, -0.5905,  1.4107, -0.9372,  0.0196, -0.6853,  1.2571,\n",
      "          0.0558,  0.8518, -1.3372, -0.2618],\n",
      "        [ 0.3481,  1.1056, -0.1298,  1.6854, -0.0120,  2.1217, -0.5399, -0.4573,\n",
      "         -0.5758,  0.9606, -0.4657,  0.9424,  1.6358,  0.3952, -0.4951, -0.4432,\n",
      "          1.0189, -0.6888,  0.0543,  0.0392],\n",
      "        [-0.4999, -0.6545,  0.4152, -0.3591,  0.8280, -0.4467,  0.3117,  0.4843,\n",
      "          0.7112,  0.1555,  0.5493, -0.9232, -1.0651, -1.9194,  0.9878, -0.2589,\n",
      "         -0.6914,  0.5554,  0.1854,  2.2337]], dtype=torch.float64) \n",
      "Batched Labels:  tensor([0, 0, 0, 0]) \n",
      "\n",
      "Batched Sample data:  tensor([[ 0.8227,  0.3741, -0.4357,  0.8138,  0.2730,  1.5023,  0.8300,  0.8439,\n",
      "          0.2203,  0.2955,  0.7180,  0.4096, -1.0425, -0.3806, -2.0533, -0.6359,\n",
      "          0.7146, -0.4867,  0.5592, -0.3763],\n",
      "        [ 0.0406,  0.0860,  0.3847,  0.6512,  0.0224, -1.0537,  0.6947, -2.1390,\n",
      "          0.0196,  0.1502,  1.0783, -0.2382, -0.7576, -0.0995, -0.4650,  0.4702,\n",
      "          0.5466,  0.8979,  0.4292,  0.5415],\n",
      "        [-0.4042,  0.4184, -0.0790, -1.6120,  1.3580, -1.2296, -0.3859, -0.3540,\n",
      "         -1.4232,  0.3166,  1.7758,  0.0884, -1.7006, -0.4336,  1.5888,  0.2989,\n",
      "          1.1238, -0.8017, -0.1060,  1.5185],\n",
      "        [ 1.1297, -0.3252, -0.0982, -0.2811, -1.7882, -1.0044,  0.7228, -0.0208,\n",
      "          0.9322, -1.6791,  0.5749,  0.3895,  1.0842, -0.2791, -0.1995,  0.5252,\n",
      "         -2.1583,  1.0223,  1.6743,  1.0277]], dtype=torch.float64) \n",
      "Batched Labels:  tensor([0, 0, 1, 1]) \n",
      "\n",
      "Batched Sample data:  tensor([[ 0.1517, -0.5585,  0.4997, -0.5046, -2.0453,  0.8505, -0.4742, -1.4410,\n",
      "          0.0437,  2.2300, -2.2281,  0.6371, -1.5842, -1.7969,  1.1460,  1.5890,\n",
      "         -0.7755, -1.5575,  0.6434, -1.4952],\n",
      "        [-0.7012,  0.8924, -0.7332, -0.0322, -0.9196, -0.5151, -1.8946, -2.2615,\n",
      "          0.0102,  1.5509, -1.8712,  1.0518,  1.6778,  0.5439, -1.7758,  1.0482,\n",
      "          0.9174,  0.1530,  0.0075,  0.3518],\n",
      "        [-0.7831, -1.9353, -0.3087, -0.9513,  0.4289, -0.9430, -1.5155,  1.6398,\n",
      "         -0.2934, -1.4539, -0.0245, -1.1225,  0.8676,  0.3211,  0.1759,  0.7531,\n",
      "         -0.0696,  0.0694, -0.7616, -0.1737],\n",
      "        [ 0.2867,  1.4241, -0.5073, -0.1450, -0.2410,  0.2202,  0.3917, -0.0588,\n",
      "          0.5082, -0.4576, -0.3319,  0.0553,  0.0073,  0.8213, -1.1285, -0.4089,\n",
      "          0.4639, -0.1480, -0.6640,  0.8275]], dtype=torch.float64) \n",
      "Batched Labels:  tensor([0, 1, 1, 0]) \n",
      "\n",
      "Batched Sample data:  tensor([[-1.1529,  0.1329, -0.6767,  0.6638,  0.0496, -0.9445,  0.0377,  1.4352,\n",
      "         -0.0596, -0.8957,  1.6930,  1.2646,  1.5611, -1.4570, -1.0804, -0.5753,\n",
      "         -0.0913, -1.6218, -1.5475,  0.4344],\n",
      "        [ 0.9699,  0.2814, -0.3581,  1.3795,  0.0764,  0.2677, -0.8697,  0.6446,\n",
      "         -0.3372, -0.3511,  1.0517,  0.7973, -1.9881, -0.5296,  0.7155,  0.2186,\n",
      "          1.5277,  0.3572, -0.2974,  2.7415],\n",
      "        [-1.1484,  0.4494, -1.1598, -1.6401,  0.1082,  0.1808,  3.6303, -1.0099,\n",
      "         -0.0947, -1.0156,  0.9630,  0.6655, -0.5889, -0.1997, -1.2763, -0.5315,\n",
      "         -1.9184, -0.7530, -0.3119,  0.7248],\n",
      "        [ 0.7559, -0.7811,  0.1698,  0.3536,  0.1759, -0.7211, -1.2796,  0.3797,\n",
      "         -0.5412, -0.1760,  1.1669,  1.0208, -1.2552, -1.0304,  0.2261, -1.9716,\n",
      "          1.1315, -0.3068, -0.3009, -0.2583]], dtype=torch.float64) \n",
      "Batched Labels:  tensor([0, 0, 1, 1]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let us define a dataloader that generates us a sample, label tuple\n",
    "# with 4 samples. We also want the indices to be shuffled.\n",
    "dl_tabular = DataLoader(data, batch_size=4, shuffle=True)\n",
    "\n",
    "# in case you are interested in how many batches you dataloader contains\n",
    "# simple use len(DataLoader)\n",
    "print(f\"Number of batches: {len(dl_tabular)}\\n\")\n",
    "\n",
    "# now print the samples and labels:\n",
    "for i, (samples, labels) in enumerate(dl_tabular):\n",
    "    print(\"Batched Sample data: \", samples, \"\\nBatched Labels: \", labels, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e477fb3-86b6-4ab9-8cd7-d95e8cac22e6",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Another cool thing these DataLoaders have is that you can provide a generator seed which helps in reproducibility.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa787a02-15c8-4d58-8c8d-1f9514c72b4e",
   "metadata": {},
   "source": [
    "Now since this course is actually targeted towards Neuroimagers I want to show you the code for the ```NiftiDataset```. Simply click on the button below to expand the code block. In case you want to import some datasets from my code you can find them in ```_core.utils.datasets```.\n",
    "\n",
    "As an exercise you could try to make sense out of the code and see if you understand what it does. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53363038-e00c-43a6-8c9d-a97a97d072c7",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "import os # we are commonly working with paths, so importing os is helpful\n",
    "import numpy as np # numpy for some numeric operations and better array structures\n",
    "import nibabel as nib # nibabel is for loading nifti files\n",
    "import glob\n",
    "from torch.utils.data import Dataset # we need to inherit from the PyTorch Dataset class\n",
    "\n",
    "\n",
    "class NiftiDataset(Dataset):\n",
    "    \"\"\"\n",
    "      NiftiLoader has torch functionality to rapidly generate and load new\n",
    "      batches for training and testing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        data_dir, \n",
    "        labels, \n",
    "        n, \n",
    "        device, \n",
    "        dims=3, \n",
    "        shuffle_labels=False, \n",
    "        transform=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructor for the NiftiDataset class\n",
    "        \n",
    "        :param data_dir:        path to the data\n",
    "        :param labels:          list of class names (directories within data_dir)\n",
    "        :param n:               the number of samples to load. If \"0\" take every example in directory.\n",
    "        :param device:          the device to use (cpu|gpu)\n",
    "        :param dims:            3 to keep the dimension, 1 to flatten into vector\n",
    "        :param shuffle_labels:  in case one wants to train a null-model enable label shuffling. Using this for training\n",
    "                                should lead to a network that provides information if labels would not matter. I.e.,\n",
    "                                it should perform only at chance level.\n",
    "        :param transform:       A composition of transformation functions that should be applied to the data.\n",
    "        \"\"\"\n",
    "\n",
    "        self.device = device\n",
    "        self.classes = labels\n",
    "        self.dims = dims\n",
    "        self.transform = transform\n",
    "\n",
    "        # get the file paths and labels\n",
    "        for iLabel in range(len(labels)):\n",
    "            # look for all files in alphanumerical order in the label directory\n",
    "            file_names = sorted(glob.glob(os.path.join(data_dir, labels[iLabel], \"*.nii.gz\")))\n",
    "            # select only the requested number of files if n > 0\n",
    "            n_files = len(file_names[:n]) if n != 0 else len(file_names)\n",
    "            \n",
    "            if iLabel == 0:\n",
    "                self.data = np.array(file_names[:n_files])\n",
    "                self.labels = np.array(np.repeat(labels[iLabel], n_files))\n",
    "            else:\n",
    "                self.data = np.append(self.data, file_names[:n_files])\n",
    "                self.labels = np.append(self.labels, np.repeat(labels[iLabel], n_files))\n",
    "\n",
    "        if shuffle_labels:\n",
    "            self.labels = np.random.permutation(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"\n",
    "        load a (batch) sample. This is usually done automatically by the \n",
    "        Pytorch DataLoader class.\n",
    "        \n",
    "        :param idx: the index of the sample to load\n",
    "        :return: tuple(volume, label)\n",
    "        \"\"\"\n",
    "        \n",
    "        # make sure that there are no NaNs in the data. \n",
    "        volume = np.nan_to_num(nib.load(self.data[idx]).get_fdata())\n",
    "        \n",
    "        volume[np.isnan(volume)] = 0 # this one is in here because I am paranoid\n",
    "        \n",
    "        # sometimes nibabel retains the temporal dimension. (x, y, z, t)\n",
    "        # we do not want that so we get rid of it.\n",
    "        if len(volume.shape) > 3:\n",
    "            volume = volume.squeeze()\n",
    "\n",
    "        volume = np.expand_dims(volume, 0) if self.dims == 3 else volume.flatten()  # add the channel dimension\n",
    "        label = np.squeeze(np.where(np.array(self.labels[idx]) == np.array(self.classes)))\n",
    "\n",
    "        # In case you provide a set of transformations execute them here\n",
    "        if self.transform:\n",
    "            label = self.transform(label).to(self.device)\n",
    "            volume = self.transform(volume).float().to(self.device)\n",
    "        else:\n",
    "            label = label.to(self.device)\n",
    "            volume = volume.to(self.device)\n",
    "\n",
    "        return volume, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858f1a75-2dfc-413f-a74e-0a45949b67ce",
   "metadata": {},
   "source": [
    "That is pretty much it with the Datasets and DataLoaders. You will see many references to them in later chapters since they are one of the building blocks of PyTorch functionality. Make sure you understand what they do and how to build one for yourself."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
