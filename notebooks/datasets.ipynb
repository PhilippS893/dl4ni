{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c642d917-7d98-4071-9d1a-a906b136539c",
   "metadata": {},
   "source": [
    "# PyTorch datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aad1f52e-94f2-4cad-b67a-30125fc3d7b2",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../../fmriDEEP'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66a8583-9d4d-43ef-a1be-51aa0b9a2ee8",
   "metadata": {},
   "source": [
    "The PyTorch [dataset and dataloader](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) classes make it easy for us to use batching etc. Their tutorial is a good place to start understanding how they work and should be read alongside this tutorial here in case there are unclear things.\n",
    "\n",
    "You can use them by importing the DataLoader and Dataset packages from torch.utils.data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8de1583-79cc-48a7-bd50-1cac2b552e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0a65ca-fe79-43ce-b387-3acdefd31ce0",
   "metadata": {},
   "source": [
    "Unfortunately, I cannot provide Dataset classes for basically any usecase unless everyone follows the conventions that we follow in the Biomedical Imaging Group. \n",
    "Let us anyway look at a Dataset that I wrote to take care of Nifti files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53363038-e00c-43a6-8c9d-a97a97d072c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # we are commonly working with paths, so importing os is helpful\n",
    "import numpy as np # numpy for some numeric operations and better array structures\n",
    "import nibabel as nib # nibabel is for loading nifti files\n",
    "import glob\n",
    "from torch.utils.data import Dataset # we need to inherit from the PyTorch Dataset class\n",
    "\n",
    "\n",
    "class NiftiDataset(Dataset):\n",
    "    \"\"\"\n",
    "      NiftiLoader has torch functionality to rapidly generate and load new\n",
    "      batches for training and testing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, labels, n, device, dims=3, shuffle_labels=False, transform=None):\n",
    "        \"\"\"\n",
    "        Constructor for the NiftiDataset class\n",
    "        \n",
    "        :param data_dir:        path to the data\n",
    "        :param labels:          list of class names (directories within data_dir)\n",
    "        :param n:               the number of samples to load. If \"0\" take every example in directory.\n",
    "        :param device:          the device to use (cpu|gpu)\n",
    "        :param dims:            3 to keep the dimension, 1 to flatten into vector\n",
    "        :param shuffle_labels:  in case one wants to train a null-model enable label shuffling. Using this for training\n",
    "                                should lead to a network that provides information if labels would not matter. I.e.,\n",
    "                                it should perform only at chance level.\n",
    "        \"\"\"\n",
    "\n",
    "        self.device = device\n",
    "        self.classes = labels\n",
    "        self.dims = dims\n",
    "        self.transform = transform\n",
    "\n",
    "        # get the file paths and labels\n",
    "        for iLabel in range(len(labels)):\n",
    "            # look for all files in alphanumerical order in the label directory\n",
    "            file_names = sorted(glob.glob(os.path.join(data_dir, labels[iLabel], \"*.nii.gz\")))\n",
    "            # select only the requested number of files if n > 0\n",
    "            n_files = len(file_names[:n]) if n != 0 else len(file_names)\n",
    "            \n",
    "            if iLabel == 0:\n",
    "                self.data = np.array(file_names[:n_files])\n",
    "                self.labels = np.array(np.repeat(labels[iLabel], n_files))\n",
    "            else:\n",
    "                self.data = np.append(self.data, file_names[:n_files])\n",
    "                self.labels = np.append(self.labels, np.repeat(labels[iLabel], n_files))\n",
    "\n",
    "        if shuffle_labels:\n",
    "            self.labels = np.random.permutation(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"\n",
    "        load a (batch) sample. This is usually done automatically by the Pytorch DataLoader class.\n",
    "        \n",
    "        :param idx: the index of the sample to load\n",
    "        :return: tuple(volume, label)\n",
    "        \"\"\"\n",
    "        \n",
    "        # make sure that there are no NaNs in the data. \n",
    "        volume = np.nan_to_num(nib.load(self.data[idx]).get_fdata())\n",
    "        \n",
    "        volume[np.isnan(volume)] = 0 # this one is in here because I am paranoid\n",
    "        \n",
    "        # sometimes nibabel retains the temporal dimension. (x, y, z, t)\n",
    "        # we do not want that so we get rid of it.\n",
    "        if len(volume.shape) > 3:\n",
    "            volume = volume.squeeze()\n",
    "\n",
    "        volume = np.expand_dims(volume, 0) if self.dims == 3 else volume.flatten()  # add the channel dimension\n",
    "        label = np.squeeze(np.where(np.array(self.labels[idx]) == np.array(self.classes)))\n",
    "\n",
    "        # In case you provide a set of transformations execute them here\n",
    "        if self.transform:\n",
    "            label = self.transform(label).to(self.device)\n",
    "            volume = self.transform(volume).float().to(self.device)\n",
    "        else \n",
    "            label = label.to(self.device)\n",
    "            volume = volume.to(self.device)\n",
    "\n",
    "        return volume, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3030417-13b3-4f7e-824d-3e9d11e1f283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "# the general setup of a Dataset class\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # do your initializations here\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        # returns the length of the dataset\n",
    "        # this usually is done by taking the len() of the labels\n",
    "        #return len(self.labels)\n",
    "        pass\n",
    "    \n",
    "    def __getitem(self, idx):\n",
    "        # this is where we actually load the data and labels\n",
    "        # commonly the data and labels are returned as a tuple(data, label)\n",
    "        # return loaded_data, loaded_label\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94300ad-ce0b-415f-b743-edc14ef4f811",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
