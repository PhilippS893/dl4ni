{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d83022f-7c6f-4607-a060-493fca3cc8f8",
   "metadata": {},
   "source": [
    "(using_sweeps)=\n",
    "# Hyperparameter sweeps with Weights&Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d43fc95-2f23-46a5-861d-7297260ef86a",
   "metadata": {},
   "source": [
    "Many parameters can affect whether a neural network can learn a task and whether its predictions generalize well to an entirely new test dataset. Some of these parameters depend on the problem at hand and the data, both of which you cannot change. For example, some learning rates might work better than others. \n",
    "\n",
    "In machine learning we use a strategy in which we split up the data into training, validation, and test data sets and run hyperparameter sweeps in which we try to find the best hyperparameters such as learning rate, batch-size, number of layers etc. pp.\n",
    "\n",
    "This multifactorial search can be tedious and it is easy to get lost. The library weights and biases helps you with that.\n",
    "\n",
    "Remember, our goal is to train a model that a) can learn the task from the training data and that b) generalizes well, i.e. performs well on new data that have not been used during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759c3198-e35d-4878-8883-ea720ea7c79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/PhilippS893/delphi\n",
    "import sys\n",
    "sys.path.insert(1, './delphi')\n",
    "\n",
    "!pip install torchinfo \n",
    "!pip install pyyaml\n",
    "!pip install -U PyYAML\n",
    "# install weights and biases, a library that helps us tracking the results from \n",
    "# parameter sweeps\n",
    "!pip install wandb -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27b0ec1c-4998-4a44-ab24-8a2aff77415f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from delphi.utils.train_fns import standard_train\n",
    "from delphi.networks.LinearNets import SimpleLinearModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "# NEW IMPORTANT IMPORTS\n",
    "from delphi.utils.tools import compute_accuracy, convert_wandb_config, read_config\n",
    "import wandb\n",
    "\n",
    "# this variable contains information whether a GPU can be used for training. If not, we automatically use the CPU.\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b97a83-6739-4a0b-9d66-4c7f9f352284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in to your W&B account\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9e6a75-cb01-4944-a3ed-f26a7c53b91c",
   "metadata": {},
   "source": [
    "Setting all random seeds for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a614431-3752-48fa-a411-5e56f3a740f4",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "# set the random seed for reproducibility\n",
    "def set_random_seed(seed):\n",
    "    import random \n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    g = torch.Generator() # can be used in pytorch dataloaders for reproducible sample selection when shuffle=True\n",
    "    g.manual_seed(seed)\n",
    "    \n",
    "    return g\n",
    "\n",
    "g = set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4ec1e1-36f9-4765-8f55-dc74c46f7260",
   "metadata": {},
   "source": [
    "Let us already load the MNIST data such that we do not forget it before running the parameter sweep :)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51c357a8-9b9d-49ff-afc5-bb6f454bfe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the MNIST dataset\n",
    "mnist_train = MNIST('./data/', train=True, download=True if not os.path.exists('./data/MNIST') else False, transform=ToTensor())\n",
    "mnist_test = MNIST('./data/', train=False, download=False, transform=ToTensor())\n",
    "\n",
    "# create the dataloaders\n",
    "dl_train = DataLoader(mnist_train, batch_size=256, shuffle=True, generator=g)\n",
    "dl_test = DataLoader(mnist_test, batch_size=256, shuffle=True, generator=g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de765cd-935e-4349-8631-b910db089c29",
   "metadata": {},
   "source": [
    "## What are hyperparameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4b5591-f110-4350-92ec-d02e6a220d82",
   "metadata": {},
   "source": [
    "A __hyperparameter__ is a variable or setting that controls the learning process. Hyperparameters are \"known\" before the learning process begins and are not changed during training. They should never be confused with the __model parameters__ (e.g., weights and biases). \n",
    "\n",
    "Typical hyperparameters are the following:\n",
    "* the learning rate\n",
    "* the number of hidden layers \n",
    "* the number of neurons per hidden layer\n",
    "* the kernel size per convolutional layer\n",
    "* the number of channels per convolutional layer\n",
    "* the cost function\n",
    "* the optimization algorithm\n",
    "* the activation function\n",
    "* the number of epochs we use for training\n",
    "* the batch size\n",
    "* even the ratio of train/validation splits\n",
    "* etc.\n",
    "\n",
    "I am sure you get the idea that anything that changes the way a network learns is considered a hyperaparameter. Now, depending on how many hyperparameters we use the searchspace of the optimal parameter settings simply explodes. It is thus pretty much impossible to set these parameters manually. Thus we need some help in determining what the optimal parameters in our given searchspace are.\n",
    "\n",
    "This is where the [weights&biases](www.wandb.ai) (wandb) package comes into play. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c159540-76dc-482a-b4c8-b58f4566da58",
   "metadata": {},
   "source": [
    "## Using weights&biases (wandb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309ecd6e-49ce-4ae0-9caf-60a750c9ebcc",
   "metadata": {},
   "source": [
    "What you will see in this jupyterbook is quite condensed and in certain cases you may need additional information that we do not provide here yet. Thus you can check out the official documention of hyperparameter sweeps with wandb [here](https://docs.wandb.ai/guides/sweeps). \n",
    "\n",
    "How we use wandb (adapted from [docs](https://docs.wandb.ai/guides/sweeps)):\n",
    "1. Write config: Define the variables and ranges to sweep over and determine the search strategy. Wandb offers a few options:\n",
    "    * grid: run all possible combinations\n",
    "    * random: randomly choose a user supplied __n__ number of parameter combinations\n",
    "    * and Bayesian search\n",
    "2. Initialize the search: Wandb hosts a controller and coordinates between the agent(s) that execute the sweep. They can be local or distributed.\n",
    "3. Launch agent(s): If we wanted to use multiple computers, we could use the same command to execute one training process with a selected parameter combination. The agent(s) ask(s) the sweep server what hyperparameter combination to try next, and then they execute the runs.\n",
    "4. We visualize the results: we can do this locally on our computers or we can use the wandb platform to do so.\n",
    "\n",
    "Let's get into it then.\n",
    "\n",
    "First, we need to set ourselfs a goal. Let's say we want to use a ```SimpleLinearModel``` to classify handwritten digits. We saw that the default settings of the ```SimpleLinearModel``` worked quite well but we cannot be sure that those parameters yield the best results. We therefore decided to do a parameter search over the number of neurons per layer, the learning rate, and the number of epochs. \n",
    "\n",
    "There are two ways in which we can do it.\n",
    "1. Defining a python ```dict```\n",
    "2. Loading a .yaml file\n",
    "\n",
    "In the cells below you see how both ways work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3aa48b-9e2a-4e34-9426-7d7f713486ed",
   "metadata": {},
   "source": [
    "### Setting up a sweep config with a python dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1811872-4813-4023-bf03-2b27e7337d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    \"name\": \"linear-mnist-sweep\",\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\n",
    "        \"name\": \"test_acc\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"lin_neurons1\": {\n",
    "            \"values\": [512, 256, 128, 64, 32, 16, 8] # the possible values for the first linear layer\n",
    "        },\n",
    "        \"lin_neurons2\": {\n",
    "            \"values\": [512, 256, 128, 64, 32, 16, 8] # the possible values for the second linear layer\n",
    "        },\n",
    "        \"lin_neurons3\": {\n",
    "            \"values\": [512, 256, 128, 64, 32, 16, 8] # the possible values for the third linear layer\n",
    "        },\n",
    "        \"learning_rate\": { # describes the range of possible values for the learning rate\n",
    "            \"min\": .0001,\n",
    "            \"max\": .1\n",
    "        },\n",
    "        \"epochs\": {\n",
    "            \"values\": [5, 10, 20, 30] # the possible values for how many epochs to train the network\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d01bf3-0842-4cbd-950d-b110f1fb4ca7",
   "metadata": {},
   "source": [
    "### Setting up a sweep config using a yaml file"
   ]
  },
  {
   "cell_type": "raw",
   "id": "63b92b6c-e526-46c8-8545-657630b00302",
   "metadata": {},
   "source": [
    "#what the mnist_sweep_config.yaml looks like\n",
    "name: my-test-sweep\n",
    "method: random\n",
    "metric:\n",
    "    name: test_acc\n",
    "parameters:\n",
    "    lin_neurons1: # the possible values for the first linear layer\n",
    "        values: [512, 256, 128, 64, 32, 16, 8]\n",
    "    lin_neurons2: # the possible values for the second linear layer\n",
    "        values: [512, 256, 128, 64, 32, 16, 8]\n",
    "    lin_neurons3: # the possible values for the third linear layer\n",
    "        values: [512, 256, 128, 64, 32, 16, 8]\n",
    "    learning_rate: # describes the range of possible values for the learning rate\n",
    "        min: .0001\n",
    "        max: .1\n",
    "    epochs: # the possible values for how many epochs to train the network\n",
    "        values: [5, 10, 20, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54b904c5-7e03-43ff-89b5-0f7c0f53581f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'linear-mnist-sweep', 'method': 'random', 'metric': {'name': 'test_acc'}, 'parameters': {'lin_neurons1': {'values': [512, 256, 128, 64, 32, 16, 8]}, 'lin_neurons2': {'values': [512, 256, 128, 64, 32, 16, 8]}, 'lin_neurons3': {'values': [512, 256, 128, 64, 32, 16, 8]}, 'learning_rate': {'min': 0.0001, 'max': 0.1}, 'epochs': {'values': [5, 10, 20, 30]}}}\n"
     ]
    }
   ],
   "source": [
    "sweep_config = read_config(\"mnist_sweep_config.yaml\")\n",
    "print(sweep_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdba8f8-4c9b-4c67-857b-a00c2024e11f",
   "metadata": {},
   "source": [
    "You probably noticed that this ```dict``` or config is not in the same format as the ```dict``` or config we need to configure our neural networks. \n",
    "Unfortunately, [wandb](www.wandb.ai) does not yet support nested values in hyperparameter searches (at least not to my knowledge). But do not be alarmed, I took care of this issue for now by writing a converter method called ```convert_wandb_config```. You can find it in the ```_core.utils.tools``` package. \n",
    "\n",
    "You will see this function in action in the sections below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088c209a-d0ee-461b-a24b-06a00c4fa303",
   "metadata": {},
   "source": [
    "## Setting up the sweep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b24409-e3b1-4118-9d10-c372fb3744f7",
   "metadata": {},
   "source": [
    "We are almost there.\n",
    "\n",
    "Wandb also requires you to set a function for your agents to call. At least in a jupyternotebook like this one it does.\n",
    "\n",
    "What you will find in the next code sections are two new functions: ```train_net()``` and ```run_train()```\n",
    "\n",
    "We will now look in detail what each of them does:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c075169-e75a-4609-963e-a2894bf93dfd",
   "metadata": {},
   "source": [
    "In the new ```train_net()``` function I defined below you should notice, that this function works now for any network you supply to it. Additionally, there is something new in there: the ```wandb.log()``` function which takes a dict with loss and accuracy scores as arguments. This function is part of the weights&biases package and logs and creates plots from the values we supply to it in real-time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "289c0b12-ef02-453c-8add-4bd125ed9d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(model, n_epochs, lr, logwandb=True):\n",
    "\n",
    "    # loop for the above set number of epochs\n",
    "    for epoch in range(0, n_epochs):\n",
    "\n",
    "        # THIS IS WHERE THE MAGIC HAPPENS\n",
    "        # calling the model.fit() function will execute the 'standard_train' function as defined above.\n",
    "        train_loss, train_stats = model.fit(dl_train, lr=lr, device=DEVICE)\n",
    "        train_acc = compute_accuracy(train_stats[:, -1], train_stats[:, -2])\n",
    "\n",
    "        # for validating or testing set the network into evaluation mode such that layers like dropout are not active\n",
    "        with torch.no_grad():\n",
    "            test_loss, test_stats = model.fit(dl_test, device=DEVICE, train=False)\n",
    "            test_acc = compute_accuracy(test_stats[:, -1], test_stats[:, -2])\n",
    "\n",
    "        print('epoch=%03d, train_loss=%1.3f, train_acc=%1.3f, test_loss=%1.3f, test_acc=%1.3f' % \n",
    "             (epoch, train_loss, train_acc, test_loss, test_acc))\n",
    "\n",
    "        # LOG PARAMETERS WITH WANDB\n",
    "        # Please keep in mind that the code below might be better placed somewhere else\n",
    "        # in case you want to use this function without weights and biases or use the\n",
    "        # logwandb flag like here\n",
    "        if logwandb:\n",
    "            wandb.log({\n",
    "                \"train_loss\": train_loss,\n",
    "                \"train_acc\": train_acc,\n",
    "                \"test_loss\": test_loss,\n",
    "                \"test_acc\": test_acc,\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9d9fde-af08-4404-bc64-9e285d88bb34",
   "metadata": {},
   "source": [
    "The ```run_train()``` function defined below might seem a bit redundant. Even though the ```train_net()``` function implemented above could be adapted with all the code below, it is best to separate as much functionality as much as possible. The way I programmed it now allows me to use the ```train_net()``` function in many different approaches. Whereas the ```run_train()``` function is currently specific for the ```SimpleLinearModel``` class. It is also the function I supply to the sweep-agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b079f7d4-3cf2-44c0-99b2-5ead5a1d20f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training function with the wandb init\n",
    "def run_train():\n",
    "    \n",
    "    # here we initialize weights&biases. \n",
    "    with wandb.init() as run:\n",
    "        \n",
    "        #Within this context we have access to the parameters the agent chose.\n",
    "        #It would look something like this:\n",
    "        #wandb.config.epochs = 5\n",
    "        #wandb.config.lin_neurons1 = 512\n",
    "        #wandb.config.lin_neurons2 = 8\n",
    "        #wandb.config.lin_neurons3 = 128\n",
    "        #wandb.config.learning_rate = 0.00791742\n",
    "        \n",
    "        # here's the promised conversion of the wandb.config\n",
    "        # this results into a dict that contains key-value pairs that we can use to configure our network:\n",
    "        # converted_config['lin_neurons'] = [512, 8, 128]\n",
    "        converted_config = convert_wandb_config(wandb.config, SimpleLinearModel._REQUIRED_PARAMS)\n",
    "        \n",
    "        model = SimpleLinearModel(784, 10, converted_config)\n",
    "        \n",
    "        # We do not necessarily need this line but it is nice to update the config.\n",
    "        wandb.config.update(model.config, allow_val_change=True)\n",
    "        \n",
    "        # now train the netwok, yay!\n",
    "        train_net(model, wandb.config.epochs, wandb.config.learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957a2e7f-9a6b-4efc-b6c3-f48675f276a8",
   "metadata": {},
   "source": [
    "It is now time to create the sweep and thus the central controller:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aacee376-bf15-4d74-aec7-e357555f4acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already ran. Skipping to save time.\n"
     ]
    }
   ],
   "source": [
    "%%script echo Already ran. Skipping to save time.\n",
    "# set the wandb sweep config\n",
    "#os.environ['WANDB_MODE'] = 'offline'\n",
    "os.environ['WANDB_ENTITY'] = \"philis893\" # this is my wandb account name. This can also be a group name, for example\n",
    "os.environ['WANDB_PROJECT'] = \"test-jupytersweep\" # this is simply the project name where we want to store the sweep logs and plots\n",
    "sweep_id = wandb.sweep(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d70da9fe-6bf5-497f-91cf-230b1648edf2",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already ran.\n"
     ]
    }
   ],
   "source": [
    "%%script echo Already ran.\n",
    "count = 20\n",
    "wandb.agent(sweep_id, function=run_train, count=count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2e85dc-741a-4cdb-94da-d00b7004ea7c",
   "metadata": {},
   "source": [
    "Checkout: https://wandb.ai/philis893/test-jupytersweep/sweeps/ybwt9udl?workspace=user-philis893"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b9c009-20d3-4c60-9e33-6abfcb71571f",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf11de0-b433-4b57-962a-1d00bf120fd1",
   "metadata": {},
   "source": [
    "Try running a hyperparameter sweep over different hyperparameters for the ```Simple2dCnnClassifier```."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
